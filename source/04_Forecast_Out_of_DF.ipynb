{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.renderers.enable('html')\n",
    "\n",
    "plt.style.use('mpl20')\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "matplotlib.rcParams['figure.figsize'] = 15, 5\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FORECAST_PERIOD = 24 * 3 # 3 days\n",
    "DECOMPOSED_SHIFT = FORECAST_PERIOD // 2 + (FORECAST_PERIOD // 2) % 24  # round to the closest number of days\n",
    "\n",
    "FEATURES_LAGS = [24 * 2 + 24 * i for i in range(1, 4)]\n",
    "RADIATION_LAGS = [24 * 2 + 24 * i for i in range(1, 4)]\n",
    "RECENT_RADIATION_LAGS = [i for i in range(1, 6)]\n",
    "MAX_DF_SHIFT = max(FEATURES_LAGS + RADIATION_LAGS)\n",
    "HOME_DIR = '/content/drive/MyDrive/Colab Notebooks/UCU_ML_2022/UCU_Machine_Learning_Course_Project'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the custom model\n",
    "dir_path = HOME_DIR + '/results/'  # for Google Colab\n",
    "# dir_path = os.getcwd() + '/'\n",
    "reconstructed_model = load_model(dir_path + 'custom_model_v3.h5')\n",
    "\n",
    "# Load MinMaxScaler.\n",
    "# Since we used it on training, validation and testing datasets, it is better also use the same on the new coming data\n",
    "delta_scaler = joblib.load(HOME_DIR + '/results/df_scaler_v3.pkl')\n",
    "\n",
    "# Import trend and seasonality to use them during forecasting\n",
    "multiplicative_decomposed_trend = pd.read_csv(HOME_DIR + '/results/multiplicative_decomposed_trend_v1.csv',\n",
    "                                              header = 0, index_col = 0, squeeze = True)\n",
    "multiplicative_decomposed_seasonal = pd.read_csv(HOME_DIR + '/results/multiplicative_decomposed_seasonal_v1.csv',\n",
    "                                                 header = 0, index_col = 0, squeeze = True)\n",
    "\n",
    "# For Google Colab here is path to dataset on Google Drive\n",
    "hourly_radiation_df = pd.read_csv(HOME_DIR + '/data/dataset1_HourlySolarRadiationProcessed.csv')\n",
    "# hourly_radiation_df = pd.read_csv(os.path.join(\"..\", \"data\", \"dataset1_HourlySolarRadiationProcessed.csv\"))\n",
    "hourly_radiation_df['Hourly_DateTime'] = pd.to_datetime(hourly_radiation_df['Hourly_DateTime'])\n",
    "hourly_radiation_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "delta_features = hourly_radiation_df[-FORECAST_PERIOD:].copy()\n",
    "forecast_datetime_range = delta_features['Hourly_DateTime']\n",
    "\n",
    "delta_features = pd.concat([delta_features, delta_features])\n",
    "delta_features.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_extrapolated_trend(trend):\n",
    "    X = trend.index[:, np.newaxis]\n",
    "    y = trend.values[:, np.newaxis]\n",
    "\n",
    "    poly_reg = PolynomialFeatures(degree=3)\n",
    "    X_poly = poly_reg.fit_transform(X)\n",
    "    pol_reg = LinearRegression()\n",
    "    pol_reg.fit(X_poly, y)\n",
    "\n",
    "    predict_idx = np.array([trend.index[-1] + i for i in range(1, DECOMPOSED_SHIFT + 1 * FORECAST_PERIOD + 1)])\n",
    "    # predict_idx = np.array([trend.index[-1] + i for i in range(1, 1 * FORECAST_PERIOD + 1)])\n",
    "    X_actual_and_forecast = np.concatenate((X, predict_idx[:, np.newaxis]))\n",
    "    polynomial_trend_prediction = pol_reg.predict(poly_reg.fit_transform(X_actual_and_forecast))\n",
    "\n",
    "    # Visualizing the Polymonial Regression results\n",
    "    plt.scatter(X, y, color='red')\n",
    "    plt.plot(X_actual_and_forecast, polynomial_trend_prediction, color='blue')\n",
    "    plt.ylabel('Production')\n",
    "    plt.title('Extrapolation of trend')\n",
    "    plt.show()\n",
    "\n",
    "    return polynomial_trend_prediction.flatten()\n",
    "\n",
    "\n",
    "def extrapolate_seasonality(seasonality, num_periods):\n",
    "    new_seasonality = pd.concat([seasonality, seasonality[:num_periods * 24]])\n",
    "    new_seasonality.index = range(0, len(new_seasonality))\n",
    "    plt.plot(new_seasonality)\n",
    "    plt.title('Extrapolation of seasonality')\n",
    "    plt.show()\n",
    "    return new_seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_feature_df_for_delta(df, df_scaler):\n",
    "    \"\"\"\n",
    "    Create dataframe of feature to train models\n",
    "\n",
    "    :return: normalized feature numpy arrays and scaler instance which was used for normalizations,\n",
    "        in the future steps it will be used to return to original values from normalized\n",
    "    \"\"\"\n",
    "    feature_lags = [24 * i for i in range(1, 4)]\n",
    "    radiation_lags = [24 * i for i in range(1, 4)]\n",
    "    recent_radiation_lags = [i for i in range(1, 6)]\n",
    "    feature_df = process_delta_data(df, feature_lags, radiation_lags, recent_radiation_lags)\n",
    "    feature_df.reset_index(drop=True, inplace=True)\n",
    "    technical_df = pd.Series([1 for _ in range(feature_df.shape[0])])\n",
    "    full_df = pd.concat([feature_df, technical_df], axis=1)\n",
    "    scaled_full_df = df_scaler.transform(full_df)\n",
    "\n",
    "    return scaled_full_df[:, :-1]\n",
    "\n",
    "\n",
    "def reshape_for_model(model_name, dataset):\n",
    "    if 'LSTM' in model_name or\\\n",
    "            'RNN' in model_name or\\\n",
    "            'Conv1d' in model_name:\n",
    "\n",
    "        if 'LSTM' in model_name:\n",
    "            # reshape input to be 3D [samples, features, timesteps]\n",
    "            dataset = dataset.reshape((dataset.shape[0], 1, dataset.shape[1]))\n",
    "\n",
    "        else:\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], 1))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def reshape_test_set_for_model(model_name, test_X):\n",
    "    if 'LSTM' in model_name or\\\n",
    "            'RNN' in model_name or\\\n",
    "            'Conv1d' in model_name:\n",
    "\n",
    "        if 'LSTM' in model_name:\n",
    "            test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "        else:\n",
    "            test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n",
    "\n",
    "    return test_X\n",
    "\n",
    "\n",
    "def test_model_with_transform(model, test_X, model_name, first_row_idx):\n",
    "    \"\"\"\n",
    "\n",
    "    Use previous predicted value to construct our feature dataframe and use it for next value prediction\n",
    "    \"\"\"\n",
    "    yhat = []\n",
    "\n",
    "    test_X = reshape_test_set_for_model(model_name, test_X)\n",
    "\n",
    "    # Take first row for which we start to forecast.\n",
    "    # Note that first_row_idx param can be also not equal to zero,\n",
    "    # for example, it can also be the last index in test_X\n",
    "    test_row = test_X[first_row_idx]\n",
    "    prod_lags = test_row[-len(RECENT_RADIATION_LAGS):]\n",
    "    test_row = test_row.reshape((1, len(test_row)))\n",
    "    test_row = reshape_for_model(model_name, test_row)\n",
    "    for i in range(len(test_X)):\n",
    "        yhat_one_pred = model.predict(test_row)\n",
    "        yhat.append(yhat_one_pred[0])\n",
    "\n",
    "        test_row = test_X[i]\n",
    "\n",
    "        prod_lags = np.roll(prod_lags, 1)\n",
    "        prod_lags[0] = yhat_one_pred\n",
    "        new_test_row = np.concatenate((test_row[:-len(RECENT_RADIATION_LAGS)], prod_lags), axis=0)\n",
    "        test_row = new_test_row.reshape((1, len(new_test_row)))\n",
    "        test_row = reshape_for_model(model_name, test_row)\n",
    "\n",
    "    yhat = np.array(yhat)\n",
    "    yhat[yhat<0] = 0\n",
    "    return yhat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_delta_data(df, features_lags, radiation_lags, recent_radiation_lags):\n",
    "    \"\"\"\n",
    "    Create a feature dataframe, which will be used for training ML and DL models\n",
    "\n",
    "    :return: tuple, where first element is dataframe of features filtered from NaN rows, which we got as result of lags for different dataframe columns;\n",
    "                          second element is a timeseries filtered from NaN entries\n",
    "    \"\"\"\n",
    "    features_df = df.copy()\n",
    "    features_df.drop(['Year', 'Day',\n",
    "                      'Month',  # we do not have data for the whole year, hence we need to drop 'Month' column\n",
    "                      'Hour', 'Log_Radiation'], axis=1, inplace=True)\n",
    "    if 'Hourly_DateTime' in features_df.columns:\n",
    "        features_df.drop(['Hourly_DateTime'], axis=1, inplace=True)\n",
    "\n",
    "    # Choose features which has good correlation or good logic causation for solar radiation\n",
    "    feature_columns = ['Temperature', 'Pressure', 'Humidity', 'ZenithDeviation',\n",
    "                       'WindDirection(Degrees)', 'Speed']\n",
    "\n",
    "    print('process_data(): features_df.shape[0] -- ', features_df.shape[0])\n",
    "    for feature_column_name in feature_columns:\n",
    "        for lag in features_lags:\n",
    "            temp = np.concatenate((np.array([np.nan for _ in range(lag)]), features_df[feature_column_name].values[:-lag]))\n",
    "            features_df[f'{feature_column_name}_lag_{lag}'] = temp\n",
    "\n",
    "    # Take radiation lags as one of the features\n",
    "    # Notice that here lags are more than our target forecast period\n",
    "    for lag in radiation_lags:\n",
    "        temp = np.concatenate((np.array([np.nan for _ in range(lag)]), features_df['Radiation'].values[:-lag]))\n",
    "        features_df[f'Radiation_lag_{lag}'] = temp\n",
    "\n",
    "    # Take last values, which we forecasted, and use them as features also\n",
    "    for lag in recent_radiation_lags:\n",
    "        temp = np.concatenate((np.array([np.nan for _ in range(lag)]), features_df['Radiation'].values[:-lag]))\n",
    "        features_df[f'Radiation_lag_{lag}'] = temp\n",
    "\n",
    "    features_df.fillna(features_df.mean(), inplace=True)\n",
    "\n",
    "    # And finally drop rainfalls\n",
    "    features_df.drop(feature_columns, axis=1, inplace=True)\n",
    "    features_df.drop(['Radiation'], axis=1, inplace=True)\n",
    "\n",
    "    print('process_data(): features_df.shape -- ', features_df.shape)\n",
    "    print('max(features_lags + radiation_lags) -- ', max(features_lags + radiation_lags))\n",
    "\n",
    "    return features_df[max(features_lags + radiation_lags):]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_out_of_df(test_model, df_scaler, original_df, test_df, forecast_datetime_range, model_name):\n",
    "    full_df_X = create_feature_df_for_delta(test_df, df_scaler)\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    full_df_X = full_df_X.reshape((full_df_X.shape[0], full_df_X.shape[1], 1))\n",
    "\n",
    "    yhat = test_model_with_transform(test_model, full_df_X, model_name, -1)\n",
    "    full_df_X = full_df_X.reshape((full_df_X.shape[0], full_df_X.shape[1]))\n",
    "\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((full_df_X, yhat), axis=1)\n",
    "    inv_yhat = df_scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "\n",
    "    # TODO\n",
    "    predicted_trend = extrapolate_seasonality(multiplicative_decomposed_seasonal, 3)[-FORECAST_PERIOD:]\n",
    "    predicted_seasonality = get_extrapolated_trend(multiplicative_decomposed_trend)[-FORECAST_PERIOD:]\n",
    "\n",
    "    print('inv_yhat.shape[0] -- ', inv_yhat.shape[0])\n",
    "    print('predicted_trend.shape[0] -- ', predicted_trend.shape[0])\n",
    "    print('predicted_seasonality.shape[0] -- ', predicted_seasonality.shape[0])\n",
    "    plt.plot(inv_yhat)\n",
    "    plt.show()\n",
    "\n",
    "    model_prediction_initial_series = inv_yhat * predicted_trend * predicted_seasonality\n",
    "\n",
    "    # start_idx = len(original_df[:-DECOMPOSED_SHIFT])\n",
    "    start_idx = len(original_df)\n",
    "    end_idx = start_idx + FORECAST_PERIOD\n",
    "    print('start_idx -- ', start_idx)\n",
    "    print('FORECAST_PERIOD -- ', FORECAST_PERIOD)\n",
    "    print('end_idx -- ', end_idx)\n",
    "    SHIFT = 0\n",
    "    # x_indices = range(start_idx, end_idx)\n",
    "    print(\"model_prediction_initial_series -- \", model_prediction_initial_series.values)\n",
    "\n",
    "    # # add + 1 to make consistent plot\n",
    "    # plt.plot(hourly_radiation_reset_df.Radiation[:-SHIFT], color='blue', label=\"initial time series\")\n",
    "    # plt.plot(x_indices, model_prediction_initial_series, color='red', label=\"forecast\")\n",
    "    # plt.legend(loc='upper left')\n",
    "    # plt.title(f'{model_name} {best_model_params} Prediction on Test Dataset')\n",
    "    # plt.grid(alpha=0.3)\n",
    "    # plt.show()\n",
    "\n",
    "    # add + 1 to make consistent plot\n",
    "    before_lag = 14 * 24\n",
    "    # plt.plot(original_df['Hourly_DateTime'][start_idx + SHIFT + 1 - before_lag: start_idx + SHIFT + 1],\n",
    "    #           original_df['Radiation'][start_idx + SHIFT + 1 - before_lag: start_idx + SHIFT + 1], color='blue', label=\"last days original radiation\")\n",
    "    # plt.plot(original_df['Hourly_DateTime'][start_idx + SHIFT: end_idx + SHIFT],\n",
    "    #          model_prediction_initial_series, color='red', label=\"forecast\")\n",
    "\n",
    "    plt.plot(original_df['Hourly_DateTime'][start_idx + SHIFT + 1 - before_lag: start_idx + SHIFT + 1],\n",
    "             original_df['Radiation'][start_idx + SHIFT + 1 - before_lag: start_idx + SHIFT + 1], color='blue', label=\"last days original radiation\")\n",
    "    plt.plot(forecast_datetime_range, model_prediction_initial_series, color='red', label=\"forecast\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'Forecast on 3 days ahead')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Before it run process_data, get_train_validation_test_splits_for_final_test, create_feature_df_for_stationary\n",
    "# + multiplicative_decomposed_seasonal + multiplicative_decomposed_trend\n",
    "predict_out_of_df(reconstructed_model, delta_scaler, hourly_radiation_df, delta_features, forecast_datetime_range, model_name='Conv1d')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}